import pandas as pd
import networkx as nx
from collections import OrderedDict

class sv_graph:
    def __init__(self, data):
        self.data   = data.copy()
        self.hub_id = self.data.iloc[0, 0]

    def graph_generator(self):
        G = nx.Graph()
        for _, row in self.data.iterrows():
            u, v, w = str(row.u), str(row.v), float(row.weight_)
            G.add_edge(u, v, weight=w)
        return G

    def cluster_classi(self, km):
        if km >= 10:
            return "C1"
        if 3 < km < 10:
            return "C2"
        return "C3"

    def cc_info(self):
        G     = self.graph_generator()
        comps = list(nx.connected_components(G))
        rows  = []

        for cid, nodes in enumerate(comps, start=1):
            sub     = G.subgraph(nodes)
            size_km = round(sub.size(weight="weight") / 1000, 2)
            cls     = self.cluster_classi(size_km)
            chain   = "Yes" if max(dict(sub.degree()).values()) == 2 else "No"

            details = []
            for u, v, w in sub.edges(data="weight"):
                mask = (
                    ((self.data.u.astype(str) == u) & (self.data.v.astype(str) == v)) |
                    ((self.data.u.astype(str) == v) & (self.data.v.astype(str) == u))
                )
                fps = (
                    self.data.loc[mask, ["fprint", "weight_"]]
                             .drop_duplicates()
                             .itertuples(index=False, name=None)
                )
                for fpt, wt in fps:
                    details.append((u, v, float(w), fpt))

            rows.append({
                "hub_id":         self.hub_id,
                "cluster_id":     cid,
                "cluster_classi": cls,
                "size_km":        size_km,
                "edge_count":     sub.number_of_edges(),
                "node_count":     sub.number_of_nodes(),
                "chain_test":     chain,
                "details":        details
            })

        return pd.DataFrame(rows)

class Initiator:
    def __init__(self, df):
        self.df = df.rename(columns={"driving_hub_id": "hub_id"})
        self.df.u      = self.df.u.astype(str)
        self.df.v      = self.df.v.astype(str)
        self.df.weight_= self.df.weight_.astype(float)

    def summary_df(self):
        cc_list = []
        for hub in self.df.hub_id.unique():
            sg = sv_graph(self.df[self.df.hub_id == hub])
            df_cc = sg.cc_info()
            df_cc["hub_id"] = hub
            cc_list.append(df_cc)
        df_cc = pd.concat(cc_list, ignore_index=True)

        rows = []
        for hub, grp in df_cc.groupby("hub_id"):
            country = hub.split("-")[1].split(".")[0]
            row = {"hub_id": hub, "country_code": country}

            for cls in ["C1", "C2", "C3"]:
                sub = grp[grp.cluster_classi == cls]
                # sum kms, count clusters
                row[f"{cls}_kms"]   = sub["size_km"].sum()
                row[f"{cls}_count"] = len(sub)

                # preserve order & dedupe segments
                raw_segs = []
                raw_fps  = []
                for detail in sub.details:
                    for (u, v, w, f) in detail:
                        raw_segs.append((u, v))
                        raw_fps.append((f, w))
                unique_segs = list(OrderedDict.fromkeys(raw_segs))
                unique_fps  = list(OrderedDict.fromkeys(raw_fps))

                row[f"{cls}_segments"]        = unique_segs
                row[f"{cls}_list_of_fprints"]  = unique_fps
                row[f"{cls}_no_of_fprint"]     = len(unique_fps)

            rows.append(row)

        return pd.DataFrame(rows)

    def details_df(self):
        rows = []
        for hub in self.df.hub_id.unique():
            sg  = sv_graph(self.df[self.df.hub_id == hub])
            cc  = sg.cc_info()
            country = hub.split("-")[1].split(".")[0]

            for _, rec in cc.iterrows():
                for (u, v, w, fpt) in rec.details:
                    rows.append({
                        "hub_id":         rec.hub_id,
                        "country_code":   country,
                        "cluster_classi": rec.cluster_classi,
                        "u":              u,
                        "v":              v,
                        "weight":         w,
                        "fprint":         fpt
                    })

        return pd.DataFrame(rows)

    def export_to_excel(self, path="hub_summary.xlsx"):
        s = self.summary_df()
        d = self.details_df()
        with pd.ExcelWriter(path, engine="xlsxwriter") as writer:
            s.to_excel(writer, sheet_name="Summary", index=False)
            d.to_excel(writer, sheet_name="Details", index=False)
        return path



# --- USAGE ---
# df = pd.read_csv("hub_data.csv")  # or however you load it
# I = Initiator(df)
# file_path = I.export_to_excel("output.xlsx")
# print("Wrote:", file_path)
#
# # Then you can use your Colab downloader:
# from colabtools import fileedit
# fileedit.download_file(file_path)
df = pd.read_csv("C:/Users/visha/Downloads/hub_data.csv")

# 2. Instantiate Initiator and export
I = Initiator(df)


I.export_to_excel("C:/Users/visha/Downloads/outputff_1_set.xlsx")
